{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9114aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a376c8",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "This is a FFN trained to add integers from -10 to + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57574571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1660 Ti with Max-Q Design is available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6bcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "N = 10000\n",
    "torch.manual_seed(42)\n",
    "\n",
    "nums_1 = torch.randint(-10, 11, (N,1), dtype=torch.float32)\n",
    "nums_2 = torch.randint(-10, 11, (N,1), dtype=torch.float32)\n",
    "\n",
    "target = nums_1 + nums_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df203766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate along columns (dim=1)\n",
    "features = torch.cat((nums_1, nums_2), dim=1)\n",
    "\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(features, target, test_size=.2)\n",
    "\n",
    "# then convert them into PyTorch Datasets (note: already converted to tensors)\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# finally, translate into dataloader objects\n",
    "batchsize    = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f9ef35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tensors[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd95084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class addMachine(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    ### input layer\n",
    "    self.input = nn.Linear(2,16)\n",
    "\n",
    "    ### hidden layers\n",
    "    self.fc1 = nn.Linear(16,32)\n",
    "    self.fc2 = nn.Linear(32,32)\n",
    "\n",
    "    ### output layer\n",
    "    self.output = nn.Linear(32,1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.relu( self.input(x) )\n",
    "    x = F.relu( self.fc1(x) )\n",
    "    x = F.relu( self.fc2(x) )\n",
    "    return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ebab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 5.,  3.],\n",
      "        [ 8.,  2.],\n",
      "        [ 7., -8.],\n",
      "        [ 3., -6.],\n",
      "        [-2., -1.],\n",
      "        [ 7., -5.],\n",
      "        [-1.,  0.],\n",
      "        [ 9.,  4.],\n",
      "        [-6.,  7.],\n",
      "        [ 8., -5.],\n",
      "        [-3.,  4.],\n",
      "        [ 0., 10.],\n",
      "        [-1.,  1.],\n",
      "        [-7., -9.],\n",
      "        [-9., -1.],\n",
      "        [ 3., -5.]]), tensor([[  8.],\n",
      "        [ 10.],\n",
      "        [ -1.],\n",
      "        [ -3.],\n",
      "        [ -3.],\n",
      "        [  2.],\n",
      "        [ -1.],\n",
      "        [ 13.],\n",
      "        [  1.],\n",
      "        [  3.],\n",
      "        [  1.],\n",
      "        [ 10.],\n",
      "        [  0.],\n",
      "        [-16.],\n",
      "        [-10.],\n",
      "        [ -2.]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1151],\n",
       "        [ 0.1731],\n",
       "        [ 0.0340],\n",
       "        [ 0.0171],\n",
       "        [-0.0501],\n",
       "        [ 0.0123],\n",
       "        [-0.0628],\n",
       "        [ 0.2093],\n",
       "        [-0.5234],\n",
       "        [ 0.0298],\n",
       "        [-0.2872],\n",
       "        [-0.3693],\n",
       "        [-0.1021],\n",
       "        [ 0.1271],\n",
       "        [-0.1684],\n",
       "        [ 0.0047]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test model on some data from the train_loader\n",
    "littleData = next(iter(train_loader))\n",
    "\n",
    "model = addMachine()\n",
    "\n",
    "print(littleData)\n",
    "model(littleData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147c063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# global parameter\n",
    "numepochs = 1000\n",
    "\n",
    "def trainTheModel(train_loader, test_loader, model):\n",
    "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "  print(f\"Using device: {device}\") \n",
    "\n",
    "  # model = model.to(device)\n",
    "\n",
    "  # loss function and optimizer\n",
    "  lossfun = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=.001)\n",
    "\n",
    "  #losses\n",
    "  trainLoss = torch.zeros(numepochs)\n",
    "  testLoss  = torch.zeros(numepochs)\n",
    "\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # switch on training mode\n",
    "    model.train()\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "      # X, y = X.to(device), y.to(device)\n",
    "      # forward pass and loss\n",
    "      yHat = model(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "    # end of batch loop...\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    trainLoss[epochi] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    model.eval()\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    # X, y = X.to(device), y.to(device)\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = model(X)\n",
    "    testLoss[epochi] = lossfun(yHat, y)\n",
    "    print(f\"Epoch_num: {epochi}, Curr_loss: {testLoss[epochi]}\")\n",
    "  # function output\n",
    "  return trainLoss, testLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a09f538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch_num: 0, Curr_loss: 0.023433905094861984\n",
      "Epoch_num: 1, Curr_loss: 0.008440238423645496\n",
      "Epoch_num: 2, Curr_loss: 0.0030633111018687487\n",
      "Epoch_num: 3, Curr_loss: 0.0018893880769610405\n",
      "Epoch_num: 4, Curr_loss: 0.001535203424282372\n",
      "Epoch_num: 5, Curr_loss: 0.0016353437677025795\n",
      "Epoch_num: 6, Curr_loss: 0.001965461764484644\n",
      "Epoch_num: 7, Curr_loss: 0.000908282061573118\n",
      "Epoch_num: 8, Curr_loss: 0.0014230404049158096\n",
      "Epoch_num: 9, Curr_loss: 0.0005467333248816431\n",
      "Epoch_num: 10, Curr_loss: 0.011184841394424438\n",
      "Epoch_num: 11, Curr_loss: 0.0007968527497723699\n",
      "Epoch_num: 12, Curr_loss: 0.0008166541229002178\n",
      "Epoch_num: 13, Curr_loss: 0.0006043299217708409\n",
      "Epoch_num: 14, Curr_loss: 0.0004978130455128849\n",
      "Epoch_num: 15, Curr_loss: 0.0038139973767101765\n",
      "Epoch_num: 16, Curr_loss: 0.0003911943349521607\n",
      "Epoch_num: 17, Curr_loss: 0.00039861383265815675\n",
      "Epoch_num: 18, Curr_loss: 0.005843241233378649\n",
      "Epoch_num: 19, Curr_loss: 0.0004795852000825107\n",
      "Epoch_num: 20, Curr_loss: 0.001275036484003067\n",
      "Epoch_num: 21, Curr_loss: 0.0029391488060355186\n",
      "Epoch_num: 22, Curr_loss: 0.0030155035201460123\n",
      "Epoch_num: 23, Curr_loss: 0.0008871741010807455\n",
      "Epoch_num: 24, Curr_loss: 0.0001860976335592568\n",
      "Epoch_num: 25, Curr_loss: 0.00777507433667779\n",
      "Epoch_num: 26, Curr_loss: 0.00242453720420599\n",
      "Epoch_num: 27, Curr_loss: 0.0002847872383426875\n",
      "Epoch_num: 28, Curr_loss: 0.00025328024639748037\n",
      "Epoch_num: 29, Curr_loss: 0.003736424958333373\n",
      "Epoch_num: 30, Curr_loss: 0.00039719673804938793\n",
      "Epoch_num: 31, Curr_loss: 0.0034617832861840725\n",
      "Epoch_num: 32, Curr_loss: 8.983008592622355e-05\n",
      "Epoch_num: 33, Curr_loss: 0.00025352754164487123\n",
      "Epoch_num: 34, Curr_loss: 0.002706456696614623\n",
      "Epoch_num: 35, Curr_loss: 0.00023863303067628294\n",
      "Epoch_num: 36, Curr_loss: 0.0003399289271328598\n",
      "Epoch_num: 37, Curr_loss: 0.0002944855659734458\n",
      "Epoch_num: 38, Curr_loss: 0.000196644919924438\n",
      "Epoch_num: 39, Curr_loss: 0.0005565230967476964\n",
      "Epoch_num: 40, Curr_loss: 0.0021756563801318407\n",
      "Epoch_num: 41, Curr_loss: 0.00179405533708632\n",
      "Epoch_num: 42, Curr_loss: 6.66950800223276e-05\n",
      "Epoch_num: 43, Curr_loss: 0.0017821594374254346\n",
      "Epoch_num: 44, Curr_loss: 0.00026551439077593386\n",
      "Epoch_num: 45, Curr_loss: 0.0010688162874430418\n",
      "Epoch_num: 46, Curr_loss: 0.00017323387146461755\n",
      "Epoch_num: 47, Curr_loss: 0.00012066315684933215\n",
      "Epoch_num: 48, Curr_loss: 8.70671501616016e-05\n",
      "Epoch_num: 49, Curr_loss: 0.0007860357291065156\n",
      "Epoch_num: 50, Curr_loss: 0.000441283656982705\n",
      "Epoch_num: 51, Curr_loss: 0.00018555953283794224\n",
      "Epoch_num: 52, Curr_loss: 0.00015385320875793695\n",
      "Epoch_num: 53, Curr_loss: 0.0008550578495487571\n",
      "Epoch_num: 54, Curr_loss: 0.00242980127222836\n",
      "Epoch_num: 55, Curr_loss: 0.0002114125236403197\n",
      "Epoch_num: 56, Curr_loss: 0.00019845250062644482\n",
      "Epoch_num: 57, Curr_loss: 0.0002210465318057686\n",
      "Epoch_num: 58, Curr_loss: 0.020656203851103783\n",
      "Epoch_num: 59, Curr_loss: 8.457280637230724e-05\n",
      "Epoch_num: 60, Curr_loss: 0.0001895166205940768\n",
      "Epoch_num: 61, Curr_loss: 0.00021023450244683772\n",
      "Epoch_num: 62, Curr_loss: 5.751104617957026e-05\n",
      "Epoch_num: 63, Curr_loss: 0.00010479218326508999\n",
      "Epoch_num: 64, Curr_loss: 0.0006877524429000914\n",
      "Epoch_num: 65, Curr_loss: 0.00016568809223826975\n",
      "Epoch_num: 66, Curr_loss: 0.0008264837088063359\n",
      "Epoch_num: 67, Curr_loss: 0.00010499668860575184\n",
      "Epoch_num: 68, Curr_loss: 0.0008872203761711717\n",
      "Epoch_num: 69, Curr_loss: 6.107140507083386e-05\n",
      "Epoch_num: 70, Curr_loss: 0.000427832652349025\n",
      "Epoch_num: 71, Curr_loss: 0.00012317333312239498\n",
      "Epoch_num: 72, Curr_loss: 0.0002411034220131114\n",
      "Epoch_num: 73, Curr_loss: 0.0002257816377095878\n",
      "Epoch_num: 74, Curr_loss: 0.0005057202070020139\n",
      "Epoch_num: 75, Curr_loss: 2.187440804846119e-05\n",
      "Epoch_num: 76, Curr_loss: 0.011335124261677265\n",
      "Epoch_num: 77, Curr_loss: 4.2101688450202346e-05\n",
      "Epoch_num: 78, Curr_loss: 0.000541391025763005\n",
      "Epoch_num: 79, Curr_loss: 4.5676151785301045e-05\n",
      "Epoch_num: 80, Curr_loss: 0.0003387288306839764\n",
      "Epoch_num: 81, Curr_loss: 0.00015024909225758165\n",
      "Epoch_num: 82, Curr_loss: 0.00013932722504250705\n",
      "Epoch_num: 83, Curr_loss: 0.00030003389110788703\n",
      "Epoch_num: 84, Curr_loss: 0.0007035310263745487\n",
      "Epoch_num: 85, Curr_loss: 0.0004988107248209417\n",
      "Epoch_num: 86, Curr_loss: 0.0018329943995922804\n",
      "Epoch_num: 87, Curr_loss: 4.0449700463796034e-05\n",
      "Epoch_num: 88, Curr_loss: 6.705002306262031e-05\n",
      "Epoch_num: 89, Curr_loss: 9.582308121025562e-05\n",
      "Epoch_num: 90, Curr_loss: 4.3193238525418565e-05\n",
      "Epoch_num: 91, Curr_loss: 0.001032804837450385\n",
      "Epoch_num: 92, Curr_loss: 0.0006938843289390206\n",
      "Epoch_num: 93, Curr_loss: 0.0006820676499046385\n",
      "Epoch_num: 94, Curr_loss: 5.072967542218976e-05\n",
      "Epoch_num: 95, Curr_loss: 3.9844875573180616e-05\n",
      "Epoch_num: 96, Curr_loss: 0.00022185385751072317\n",
      "Epoch_num: 97, Curr_loss: 0.0016339507419615984\n",
      "Epoch_num: 98, Curr_loss: 4.759883086080663e-05\n",
      "Epoch_num: 99, Curr_loss: 0.00019384689221624285\n",
      "Epoch_num: 100, Curr_loss: 0.0014309126418083906\n",
      "Epoch_num: 101, Curr_loss: 8.853247709339485e-05\n",
      "Epoch_num: 102, Curr_loss: 7.513073069276288e-05\n",
      "Epoch_num: 103, Curr_loss: 3.458938226685859e-05\n",
      "Epoch_num: 104, Curr_loss: 0.0005504625150933862\n",
      "Epoch_num: 105, Curr_loss: 4.243732109898701e-05\n",
      "Epoch_num: 106, Curr_loss: 2.1437819668790326e-05\n",
      "Epoch_num: 107, Curr_loss: 5.6377419241471216e-05\n",
      "Epoch_num: 108, Curr_loss: 0.001080794958397746\n",
      "Epoch_num: 109, Curr_loss: 0.005851036868989468\n",
      "Epoch_num: 110, Curr_loss: 4.487156184040941e-05\n",
      "Epoch_num: 111, Curr_loss: 8.624844485893846e-05\n",
      "Epoch_num: 112, Curr_loss: 0.00018957168504130095\n",
      "Epoch_num: 113, Curr_loss: 0.00015990900283213705\n",
      "Epoch_num: 114, Curr_loss: 7.668932812521234e-05\n",
      "Epoch_num: 115, Curr_loss: 0.0004884046502411366\n",
      "Epoch_num: 116, Curr_loss: 3.826294778264128e-05\n",
      "Epoch_num: 117, Curr_loss: 2.0436411432456225e-05\n",
      "Epoch_num: 118, Curr_loss: 0.0017200327711179852\n",
      "Epoch_num: 119, Curr_loss: 0.00024339029914699495\n",
      "Epoch_num: 120, Curr_loss: 2.4591523470007814e-05\n",
      "Epoch_num: 121, Curr_loss: 0.0044592213816940784\n",
      "Epoch_num: 122, Curr_loss: 5.65708578506019e-05\n",
      "Epoch_num: 123, Curr_loss: 0.0006068149814382195\n",
      "Epoch_num: 124, Curr_loss: 2.089898225676734e-05\n",
      "Epoch_num: 125, Curr_loss: 2.36692103499081e-05\n",
      "Epoch_num: 126, Curr_loss: 3.48653229593765e-05\n",
      "Epoch_num: 127, Curr_loss: 7.114674372132868e-05\n",
      "Epoch_num: 128, Curr_loss: 0.0001757673016982153\n",
      "Epoch_num: 129, Curr_loss: 6.215238681761548e-05\n",
      "Epoch_num: 130, Curr_loss: 9.774415229912847e-05\n",
      "Epoch_num: 131, Curr_loss: 0.0008390989387407899\n",
      "Epoch_num: 132, Curr_loss: 0.0004077190242242068\n",
      "Epoch_num: 133, Curr_loss: 3.09259703499265e-05\n",
      "Epoch_num: 134, Curr_loss: 0.00023392178991343826\n",
      "Epoch_num: 135, Curr_loss: 0.0010288688354194164\n",
      "Epoch_num: 136, Curr_loss: 1.7673492038738914e-05\n",
      "Epoch_num: 137, Curr_loss: 7.198212642833823e-06\n",
      "Epoch_num: 138, Curr_loss: 0.0004904429079033434\n",
      "Epoch_num: 139, Curr_loss: 0.0005805004038847983\n",
      "Epoch_num: 140, Curr_loss: 1.9227369193686172e-05\n",
      "Epoch_num: 141, Curr_loss: 0.004663913045078516\n",
      "Epoch_num: 142, Curr_loss: 1.121372315537883e-05\n",
      "Epoch_num: 143, Curr_loss: 1.1916491530428175e-05\n",
      "Epoch_num: 144, Curr_loss: 0.00021582070621661842\n",
      "Epoch_num: 145, Curr_loss: 0.00040848719072528183\n",
      "Epoch_num: 146, Curr_loss: 5.4748423281125724e-05\n",
      "Epoch_num: 147, Curr_loss: 6.110434696893208e-06\n",
      "Epoch_num: 148, Curr_loss: 2.6071116735693067e-05\n",
      "Epoch_num: 149, Curr_loss: 1.2301449714868795e-05\n",
      "Epoch_num: 150, Curr_loss: 0.00029072986217215657\n",
      "Epoch_num: 151, Curr_loss: 0.00012280118244234473\n",
      "Epoch_num: 152, Curr_loss: 0.00021770002786070108\n",
      "Epoch_num: 153, Curr_loss: 1.0989958354912233e-05\n",
      "Epoch_num: 154, Curr_loss: 0.0009921586606651545\n",
      "Epoch_num: 155, Curr_loss: 7.154350896598771e-06\n",
      "Epoch_num: 156, Curr_loss: 1.592122862348333e-05\n",
      "Epoch_num: 157, Curr_loss: 9.172311365546193e-06\n",
      "Epoch_num: 158, Curr_loss: 6.837873115728144e-06\n",
      "Epoch_num: 159, Curr_loss: 1.5284949768101797e-05\n",
      "Epoch_num: 160, Curr_loss: 1.8285709302290343e-05\n",
      "Epoch_num: 161, Curr_loss: 1.3716558896703646e-05\n",
      "Epoch_num: 162, Curr_loss: 3.888528226525523e-06\n",
      "Epoch_num: 163, Curr_loss: 0.0001708028430584818\n",
      "Epoch_num: 164, Curr_loss: 9.863570994639304e-06\n",
      "Epoch_num: 165, Curr_loss: 3.574894435587339e-05\n",
      "Epoch_num: 166, Curr_loss: 0.00040714582428336143\n",
      "Epoch_num: 167, Curr_loss: 3.3748601708794013e-06\n",
      "Epoch_num: 168, Curr_loss: 3.3610212994972244e-05\n",
      "Epoch_num: 169, Curr_loss: 8.06645221018698e-06\n",
      "Epoch_num: 170, Curr_loss: 7.642010314157233e-05\n",
      "Epoch_num: 171, Curr_loss: 7.146027928683907e-05\n",
      "Epoch_num: 172, Curr_loss: 9.919791773427278e-05\n",
      "Epoch_num: 173, Curr_loss: 0.0007403533672913909\n",
      "Epoch_num: 174, Curr_loss: 6.499863957287744e-06\n",
      "Epoch_num: 175, Curr_loss: 0.002108643064275384\n",
      "Epoch_num: 176, Curr_loss: 8.653981240058783e-06\n",
      "Epoch_num: 177, Curr_loss: 2.866424665626255e-06\n",
      "Epoch_num: 178, Curr_loss: 9.688009413366672e-06\n",
      "Epoch_num: 179, Curr_loss: 0.00038174918154254556\n",
      "Epoch_num: 180, Curr_loss: 5.435846105683595e-06\n",
      "Epoch_num: 181, Curr_loss: 3.351809255036642e-06\n",
      "Epoch_num: 182, Curr_loss: 0.0020182279404252768\n",
      "Epoch_num: 183, Curr_loss: 0.0006089502130635083\n",
      "Epoch_num: 184, Curr_loss: 1.7631355149205774e-05\n",
      "Epoch_num: 185, Curr_loss: 5.323220102582127e-05\n",
      "Epoch_num: 186, Curr_loss: 1.4938161257305183e-05\n",
      "Epoch_num: 187, Curr_loss: 1.4858345821266994e-05\n",
      "Epoch_num: 188, Curr_loss: 5.046560545451939e-06\n",
      "Epoch_num: 189, Curr_loss: 6.391214355971897e-06\n",
      "Epoch_num: 190, Curr_loss: 1.3552622476709075e-05\n",
      "Epoch_num: 191, Curr_loss: 0.0002592824457678944\n",
      "Epoch_num: 192, Curr_loss: 3.640192517195828e-05\n",
      "Epoch_num: 193, Curr_loss: 1.8447010006639175e-05\n",
      "Epoch_num: 194, Curr_loss: 0.0002520577982068062\n",
      "Epoch_num: 195, Curr_loss: 4.881117638433352e-05\n",
      "Epoch_num: 196, Curr_loss: 0.019678965210914612\n",
      "Epoch_num: 197, Curr_loss: 5.913251243327977e-06\n",
      "Epoch_num: 198, Curr_loss: 2.4976429813250434e-06\n",
      "Epoch_num: 199, Curr_loss: 0.0001190653711091727\n",
      "Epoch_num: 200, Curr_loss: 0.0001693325029918924\n",
      "Epoch_num: 201, Curr_loss: 6.645924440817907e-05\n",
      "Epoch_num: 202, Curr_loss: 5.73204415559303e-05\n",
      "Epoch_num: 203, Curr_loss: 3.2320592708856566e-06\n",
      "Epoch_num: 204, Curr_loss: 1.909634011099115e-05\n",
      "Epoch_num: 205, Curr_loss: 0.00011622200690908358\n",
      "Epoch_num: 206, Curr_loss: 0.00013361091259866953\n",
      "Epoch_num: 207, Curr_loss: 8.06552361609647e-06\n",
      "Epoch_num: 208, Curr_loss: 4.754978363052942e-05\n",
      "Epoch_num: 209, Curr_loss: 2.915191089414293e-06\n",
      "Epoch_num: 210, Curr_loss: 4.593752964865416e-05\n",
      "Epoch_num: 211, Curr_loss: 0.0018007890321314335\n",
      "Epoch_num: 212, Curr_loss: 3.279911652498413e-06\n",
      "Epoch_num: 213, Curr_loss: 1.84718519449234e-05\n",
      "Epoch_num: 214, Curr_loss: 4.790448201674735e-06\n",
      "Epoch_num: 215, Curr_loss: 0.00010759585711639374\n",
      "Epoch_num: 216, Curr_loss: 3.461728283582488e-06\n",
      "Epoch_num: 217, Curr_loss: 1.1663206578305108e-06\n",
      "Epoch_num: 218, Curr_loss: 5.162692104931921e-05\n",
      "Epoch_num: 219, Curr_loss: 0.00017769442638382316\n",
      "Epoch_num: 220, Curr_loss: 7.16523072696873e-06\n",
      "Epoch_num: 221, Curr_loss: 7.0480323302035686e-06\n",
      "Epoch_num: 222, Curr_loss: 5.547130513150478e-06\n",
      "Epoch_num: 223, Curr_loss: 4.200357579975389e-06\n",
      "Epoch_num: 224, Curr_loss: 0.00011741029447875917\n",
      "Epoch_num: 225, Curr_loss: 7.378352165687829e-06\n",
      "Epoch_num: 226, Curr_loss: 0.00010765875776996836\n",
      "Epoch_num: 227, Curr_loss: 4.8134693315660115e-06\n",
      "Epoch_num: 228, Curr_loss: 8.846952368912753e-06\n",
      "Epoch_num: 229, Curr_loss: 4.05122182201012e-06\n",
      "Epoch_num: 230, Curr_loss: 3.951862709072884e-06\n",
      "Epoch_num: 231, Curr_loss: 0.0025205332785844803\n",
      "Epoch_num: 232, Curr_loss: 5.361781586543657e-05\n",
      "Epoch_num: 233, Curr_loss: 4.569096290651942e-06\n",
      "Epoch_num: 234, Curr_loss: 0.00012634243466891348\n",
      "Epoch_num: 235, Curr_loss: 4.851379344472662e-06\n",
      "Epoch_num: 236, Curr_loss: 2.641267201397568e-06\n",
      "Epoch_num: 237, Curr_loss: 0.001058296300470829\n",
      "Epoch_num: 238, Curr_loss: 0.00019765239267144352\n",
      "Epoch_num: 239, Curr_loss: 2.9505770271498477e-06\n",
      "Epoch_num: 240, Curr_loss: 2.882221224353998e-06\n",
      "Epoch_num: 241, Curr_loss: 7.091868610586971e-05\n",
      "Epoch_num: 242, Curr_loss: 0.00027669459814205766\n",
      "Epoch_num: 243, Curr_loss: 0.001130301272496581\n",
      "Epoch_num: 244, Curr_loss: 2.7912140012631426e-06\n",
      "Epoch_num: 245, Curr_loss: 3.3976975828409195e-06\n",
      "Epoch_num: 246, Curr_loss: 0.00014843711687717587\n",
      "Epoch_num: 247, Curr_loss: 5.55819178771344e-06\n",
      "Epoch_num: 248, Curr_loss: 0.0018688691779971123\n",
      "Epoch_num: 249, Curr_loss: 4.956595148541965e-06\n",
      "Epoch_num: 250, Curr_loss: 1.9570350559661165e-05\n",
      "Epoch_num: 251, Curr_loss: 1.9869403331540525e-05\n",
      "Epoch_num: 252, Curr_loss: 1.1972578249697108e-05\n",
      "Epoch_num: 253, Curr_loss: 1.7732360220179544e-06\n",
      "Epoch_num: 254, Curr_loss: 1.9519504348863848e-05\n",
      "Epoch_num: 255, Curr_loss: 4.066353994858218e-06\n",
      "Epoch_num: 256, Curr_loss: 0.0003682250971905887\n",
      "Epoch_num: 257, Curr_loss: 9.92817513179034e-05\n",
      "Epoch_num: 258, Curr_loss: 2.2773629098082893e-05\n",
      "Epoch_num: 259, Curr_loss: 0.0005881102406419814\n",
      "Epoch_num: 260, Curr_loss: 0.002617262303829193\n",
      "Epoch_num: 261, Curr_loss: 3.368050101926201e-06\n",
      "Epoch_num: 262, Curr_loss: 1.1390859526727581e-06\n",
      "Epoch_num: 263, Curr_loss: 0.00010112085874425247\n",
      "Epoch_num: 264, Curr_loss: 0.00031187888816930354\n",
      "Epoch_num: 265, Curr_loss: 2.669020432222169e-06\n",
      "Epoch_num: 266, Curr_loss: 0.006704556290060282\n",
      "Epoch_num: 267, Curr_loss: 2.2125727809907403e-06\n",
      "Epoch_num: 268, Curr_loss: 4.264104973117355e-06\n",
      "Epoch_num: 269, Curr_loss: 0.0016718648839741945\n",
      "Epoch_num: 270, Curr_loss: 2.2930209979676874e-06\n",
      "Epoch_num: 271, Curr_loss: 2.1699837816413492e-05\n",
      "Epoch_num: 272, Curr_loss: 1.092876004804566e-06\n",
      "Epoch_num: 273, Curr_loss: 0.0005507569876499474\n",
      "Epoch_num: 274, Curr_loss: 5.535899163078284e-06\n",
      "Epoch_num: 275, Curr_loss: 1.6916098957153736e-06\n",
      "Epoch_num: 276, Curr_loss: 1.177034846477909e-06\n",
      "Epoch_num: 277, Curr_loss: 0.00028619784279726446\n",
      "Epoch_num: 278, Curr_loss: 0.009712504222989082\n",
      "Epoch_num: 279, Curr_loss: 4.504164735408267e-06\n",
      "Epoch_num: 280, Curr_loss: 0.00037463492481037974\n",
      "Epoch_num: 281, Curr_loss: 8.18726334728126e-07\n",
      "Epoch_num: 282, Curr_loss: 5.6619242059241515e-06\n",
      "Epoch_num: 283, Curr_loss: 5.318707917467691e-05\n",
      "Epoch_num: 284, Curr_loss: 8.75628484209301e-06\n",
      "Epoch_num: 285, Curr_loss: 0.0020577283576130867\n",
      "Epoch_num: 286, Curr_loss: 7.526669651269913e-05\n",
      "Epoch_num: 287, Curr_loss: 3.001052391482517e-06\n",
      "Epoch_num: 288, Curr_loss: 0.0002652307157404721\n",
      "Epoch_num: 289, Curr_loss: 0.0001158996019512415\n",
      "Epoch_num: 290, Curr_loss: 2.75004258583067e-05\n",
      "Epoch_num: 291, Curr_loss: 0.000843479298055172\n",
      "Epoch_num: 292, Curr_loss: 1.7251779809157597e-06\n",
      "Epoch_num: 293, Curr_loss: 4.417570835357765e-06\n",
      "Epoch_num: 294, Curr_loss: 3.995632869191468e-05\n",
      "Epoch_num: 295, Curr_loss: 4.643851752916817e-06\n",
      "Epoch_num: 296, Curr_loss: 2.5085132620006334e-06\n",
      "Epoch_num: 297, Curr_loss: 2.852522129614954e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m addition_model \u001b[38;5;241m=\u001b[39m addMachine()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainTheModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddition_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m, in \u001b[0;36mtrainTheModel\u001b[0;34m(train_loader, test_loader, model)\u001b[0m\n\u001b[1;32m     26\u001b[0m batchLoss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X,y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;66;03m# X, y = X.to(device), y.to(device)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m   \u001b[38;5;66;03m# forward pass and loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m   yHat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m   loss \u001b[38;5;241m=\u001b[39m lossfun(yHat,y)\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;66;03m# backprop\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DUDL/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DUDL/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36maddMachine.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     16\u001b[0m   x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput(x) )\n\u001b[0;32m---> 17\u001b[0m   x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu( \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m )\n\u001b[1;32m     18\u001b[0m   x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x) )\n\u001b[1;32m     19\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m~/Documents/DUDL/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DUDL/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/DUDL/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "addition_model = addMachine()\n",
    "trainTheModel(train_loader, test_loader, addition_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60c1be1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0046], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = torch.tensor([8,-10], dtype=torch.float32)\n",
    "addition_model(test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DUDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
